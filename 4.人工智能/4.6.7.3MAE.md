# MAE

看本文前，请确保你已了解 BERT 的相关知识

# 前言

MAE 是一个把 BERT 的随机掩码结构拓展应用到 cv 领域的模型

目的是通过自监督训练一个通用的 cv 的 backbone 模型

## MAE 想解决的问题

cv 领域，其实预训练模型早已推广，一般是在 imagenet 上进行预训练，但是 imagenet 的图片是有局限性的，比如物体一般是在图片中间等等。并且指标也越来越难刷上去。于是作者想通过像 BERT 一样进行无监督学习来引入更多数据并降低训练成本。

那么问题来了，既然我们要学习 BERT 的随机掩码，那么我们应该对什么做 mask 呢？

因为图片不像文本，有单词这一基础单位。图片的基础单位像素在被单独拿出来的时候包含的语义信息是完全不如单词的。因为像素的语义信息与**上下左右的连续关系**很密切。于是作者采用了像 VIT 那样把图片分成好几个 patch，对 patch 做随机掩码。

# 模型结构与训练方式

看了上面的解释，相信你也有些思路了，其实这个模型很简单，只要拿随便一个抽取特征用的 backbone 模型，比如 VIT,CNN 之类的做编码器，再接一个生成图像的解码器就 ok 啦。下面讲一下作者用的模型结构：

## 具体模型结构

为了方便比较起见，作者用了 VIT-large 做编码器，多层堆叠的 transformer 做解码器，其他貌似也没什么特殊的了。

## 模型输入

在这里，作者为了加大任务的难度，扩大了被 mask 掉的比例，避免模型只学到双线性插值去修补缺的图像。作者把 75% 的 patch 进行 mask，然后放入模型训练。从下图可以看出，被 mask 的块是不进行编码的，这样也可以降低计算量，减少成本。

![](https://cdn.xyxsw.site/boxcnd7HTEFOiJxVQ3jtOpzK4ie.png)

在被保留的块通过编码器后，我们再在原先位置插入只包含位置信息的 mask 块，一起放入解码器。

## 训练方式

在通过 VIT 编码抽取特征和多层 transformer 生成图片后，我们对生成的图片做简单的 MSE 损失（就是平方损失），在训练完成后，去掉多层 transformer，留下训练好的 VIT 做 backbone，进行微调就可以处理下游任务了。这个在 BERT 里讲了，这里不再赘述。

下面是原论文给的训练结果，可以看到效果是很惊人的。（有些图我脑补都补不出来）

![](https://cdn.xyxsw.site/boxcnPWO0VWbPvCE537tf6MWu4e.png)

# 相关资料

更具体的比如模型性能对比最好还是去看原论文或者李沐老师的讲解

李沐【MAE 论文逐段精读【论文精读】】 https://www.bilibili.com/video/BV1sq4y1q77t

<Bilibili bvid='BV1sq4y1q77t'/>

原论文：[https://arxiv.org/pdf/2111.06377v2.pdf](https://arxiv.org/pdf/2111.06377v2.pdf)
